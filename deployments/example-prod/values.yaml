# Production Configuration Example
# Complete production reference for example.com
# Copy this entire file to start a new production deployment

global:
  domain: example.com
  namespace: example-prod
  environment: production

  gateway:
    name: shared-gateway
    namespace: gateway-system

  storage:
    provider: cloud-default  # Use cloud provider's default StorageClass
    className: ""  # Auto-detect from provider

# ===== ArgoCD GitOps Configuration =====
argocd:
  # Git repository URL (required for GitOps)
  repoURL: https://github.com/monobaselabs/monobase-infra.git  # Change to your fork
  targetRevision: main  # Branch to deploy from (main, staging, etc.)

# ===== CORE: Monobase API =====
api:
  enabled: true

  image:
    repository: ghcr.io/monobaselabs/api
    tag: "latest"  # Pin specific version in production
    pullPolicy: IfNotPresent

  replicas: 2  # Minimum for HA

  resources:
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      cpu: 2
      memory: 2Gi

  gateway:
    hostname: ""  # Empty = uses api.{global.domain}

  livenessProbe:
    enabled: true
    path: /health
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3

  readinessProbe:
    enabled: true
    path: /ready
    initialDelaySeconds: 10
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 2

  podDisruptionBudget:
    enabled: true
    minAvailable: 1

  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 5
    targetCPUUtilizationPercentage: 70
    targetMemoryUtilizationPercentage: 80

# ===== CORE: Monobase Account Frontend =====
account:
  enabled: true

  image:
    repository: ghcr.io/monobaselabs/api
    tag: "latest"  # Pin specific version
    pullPolicy: IfNotPresent

  replicas: 2

  resources:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi

  gateway:
    hostname: ""  # Empty = uses app.{global.domain}

  podDisruptionBudget:
    enabled: true
    minAvailable: 1

# ===== CORE: PostgreSQL Database =====
postgresql:
  enabled: true

  # Production uses replication for HA
  architecture: replication
  replicaCount: 2  # 1 primary + 1 replica

  auth:
    enabled: true
    # Credentials managed by External Secrets

  persistence:
    enabled: true
    storageClass: ""  # Auto-detect from global.storage.provider
    size: 50Gi  # Adjust based on data volume

  resources:
    requests:
      cpu: 500m
      memory: 2Gi
    limits:
      cpu: 2
      memory: 4Gi

  podDisruptionBudget:
    enabled: true
    minAvailable: 1

# ===== OPTIONAL: Valkey (Redis Cache) =====
valkey:
  enabled: true

  architecture: replication

  master:
    persistence:
      enabled: true
      storageClass: ""  # Auto-detect
      size: 8Gi

  resources:
    requests:
      cpu: 250m
      memory: 512Mi
    limits:
      cpu: 1
      memory: 1Gi

# ===== OPTIONAL: MinIO (Object Storage) =====
minio:
  enabled: false  # Disabled by default - use cloud S3 instead

  # If enabled, use distributed mode for production
  mode: distributed
  replicas: 6

  persistence:
    enabled: true
    storageClass: ""
    size: 250Gi

# ===== PRODUCTION: NO Mailpit =====
mailpit:
  enabled: false  # NEVER enable in production (use real SMTP)

# ===== External Secrets =====
externalSecrets:
  enabled: true
  provider: aws  # Options: aws, azure, gcp

# ===== Monitoring =====
monitoring:
  enabled: false  # Enable when needed

# ===== Backup (Velero) =====
backup:
  enabled: true
  provider: aws  # Options: aws, azure, gcp
  bucket: ""  # Set to your backup bucket (e.g., example-prod-backups)
  region: ""  # Set to your region (e.g., us-east-1)
  credentialSecret: velero-credentials  # Created by External Secrets

  encryption:
    enabled: true
    type: AES256
    kmsKeyId: ""  # Optional: AWS KMS key for SSE-KMS

  schedules:
    # Daily full backup (recommended for production)
    daily:
      enabled: true
      schedule: "0 2 * * *"
      retention: 720h  # 30 days

    # Hourly critical data (optional - enable for RPO <1 day)
    hourly:
      enabled: false
      schedule: "0 * * * *"
      retention: 72h

    # Weekly archive (optional - for compliance)
    weekly:
      enabled: false
      schedule: "0 3 * * 0"  # Sunday 3 AM
      retention: 2160h  # 90 days
      storageLocation: default

# ===== Security =====
networkPolicies:
  enabled: true
  defaultDeny: true

podSecurityStandards:
  enabled: true
  level: restricted

# Security tools (optional - see docs/security/ for decision guide)
security:
  kyverno:
    enabled: false  # Set true for policy enforcement (multi-team, kubectl access)

    # Kyverno configuration (applies if enabled)
    replicaCount: 3  # HA for production

    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi

    podDisruptionBudget:
      enabled: true
      minAvailable: 1

    admissionController:
      replicas: 3
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
        limits:
          cpu: 500m
          memory: 512Mi

    backgroundController:
      enabled: true
      replicas: 2
      resources:
        requests:
          cpu: 50m
          memory: 64Mi
        limits:
          cpu: 200m
          memory: 256Mi

    cleanupController:
      enabled: true
      resources:
        requests:
          cpu: 50m
          memory: 64Mi
        limits:
          cpu: 200m
          memory: 128Mi

    reportsController:
      enabled: true
      resources:
        requests:
          cpu: 50m
          memory: 64Mi
        limits:
          cpu: 200m
          memory: 256Mi

    webhooksCleanup:
      enabled: true

    podSecurityContext:
      runAsNonRoot: true
      runAsUser: 1000
      fsGroup: 1000
      seccompProfile:
        type: RuntimeDefault

    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true

    metricsService:
      create: true
      type: ClusterIP
      port: 8000
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"

    serviceMonitor:
      enabled: false  # Enable if using Prometheus Operator
      namespace: kyverno
      interval: 30s

    features:
      generateValidatingAdmissionPolicy:
        enabled: false
      logging:
        format: json
        verbosity: 2

    config:
      excludeKyvernoNamespace: true
      resourceFilters:
      - '[Event,*,*]'
      - '[*,kube-system,*]'
      - '[*,kube-public,*]'
      - '[*,kube-node-lease,*]'
      - '[*,kyverno,*]'
      - '[*,velero,*]'
      - '[*,argocd,*]'
      - '[*,cert-manager,*]'
      - '[*,external-secrets-system,*]'
      - '[*,longhorn-system,*]'
      - '[*,monitoring,*]'
      - '[*,falco,*]'
      - '[Node,*,*]'
      - '[APIService,*,*]'
      - '[TokenReview,*,*]'
      - '[SubjectAccessReview,*,*]'
      - '[SelfSubjectAccessReview,*,*]'
      - '[Binding,*,*]'
      - '[ReplicaSet,*,*]'
      - '[AdmissionReport,*,*]'
      - '[ClusterAdmissionReport,*,*]'
      - '[BackgroundScanReport,*,*]'
      - '[ClusterBackgroundScanReport,*,*]'

    admissionReports:
      enabled: true

    backgroundReports:
      enabled: true

    webhookTimeout: 10

    upgradeController:
      enabled: true

    crds:
      install: true
      annotations: {}

  falco:
    enabled: false  # Set true for runtime security monitoring (production, compliance)

    # Falco configuration (applies if enabled)
    daemonset:
      updateStrategy:
        type: RollingUpdate

    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 512Mi

    podSecurityContext:
      runAsNonRoot: true
      runAsUser: 1000
      fsGroup: 1000
      seccompProfile:
        type: RuntimeDefault

    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
        add:
        - SYS_PTRACE  # Required for eBPF
      readOnlyRootFilesystem: true
      privileged: false

    falco:
      rulesFile:
      - /etc/falco/falco_rules.yaml
      - /etc/falco/falco_rules.local.yaml
      - /etc/falco/rules.d

      priority: notice

      jsonOutput: true
      jsonIncludeOutputProperty: true
      jsonIncludeTagsProperty: true

      logStderr: true
      logSyslog: false
      logLevel: info

      webserver:
        enabled: true
        port: 8765
        prometheus_metrics_enabled: true

      syscall_event_drops:
        actions:
        - log
        - alert
        rate: 0.03333
        max_burst: 1000

      fileOutput:
        enabled: true
        keepAlive: false
        filename: /var/log/falco/events.log

      syslogOutput:
        enabled: false
        host: "syslog-server.example.com"
        port: 514
        format: json

      httpOutput:
        enabled: false
        url: ""  # Slack webhook URL
        userAgent: "falco/0.1"

      programOutput:
        enabled: false
        keepAlive: false
        program: |
          jq '{
            text: .output,
            priority: .priority,
            rule: .rule,
            time: .time
          }' | curl -X POST -H 'Content-Type: application/json' -d @- YOUR_WEBHOOK_URL

      grpc:
        enabled: false
        bindAddress: "unix:///run/falco/falco.sock"
        threadiness: 0

    driver:
      enabled: true
      kind: ebpf
      ebpf:
        hostNetwork: true
        path: /usr/src/ebpf

    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists

    nodeSelector: {}

    serviceAccount:
      create: true
      annotations: {}

    rbac:
      create: true

    podSecurityPolicy:
      create: false

    customRules: {}

    extraVolumes:
    - name: custom-rules
      configMap:
        name: falco-custom-rules

    extraVolumeMounts:
    - name: custom-rules
      mountPath: /etc/falco/rules.d
      readOnly: true

    services:
    - name: metrics
      type: ClusterIP
      ports:
      - port: 8765
        targetPort: 8765
        protocol: TCP
        name: metrics

    serviceMonitor:
      enabled: false
      interval: 30s
      scrapeTimeout: 10s

# ===== Resource Quotas =====
resourceQuotas:
  enabled: false  # Enable for multi-tenant clusters
